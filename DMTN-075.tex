\documentclass[DM,authoryear,toc]{lsstdoc}
% lsstdoc documentation: https://lsst-texmf.lsst.io/lsstdoc.html
\input{meta}

% Package imports go here.
\usepackage{unicode-math}
\usepackage{color}

% Local commands go here.
\newcommand{\todo}[1]{{\color{red}[#1]}}
\newcommand{\ft}[0]{\mathcal{F}}

\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\supp}{supp}


%If you want glossaries
%\input{aglossary.tex}
%\makeglossaries

\title{Cell-Based Coaddition}

% Optional subtitle
% \setDocSubtitle{A subtitle}

\author{%
Jim Bosch
}

\setDocRef{DMTN-075}
\setDocUpstreamLocation{\url{https://github.com/lsst-dm/dmtn-075}}

\date{\vcsDate}

% Optional: name of the document's curator
% \setDocCurator{The Curator of this Document}

\setDocAbstract{%
Design sketch and mathematics for a new approach to building coadds and constraining their inputs.
}

% Change history defined here.
% Order: oldest first.
% Fields: VERSION, DATE, DESCRIPTION, OWNER NAME.
% See LPM-51 for version number policy.
\setDocChangeRecord{%
  \addtohist{1}{YYYY-MM-DD}{Unreleased.}{Jim Bosch}
}


\begin{document}

% Create the title page.
\maketitle
% Frequently for a technote we do not want a title page  uncomment this to remove the title page and changelog.
% use \mkshorttitle to remove the extra pages

\section{Likelihood Coadds}

\subsection{Derivation and starting assumptions}

We start by writing the log likelihood of an arbitrary model of the local sky, $α$, given multiple images $z_j$ with differing PSFs $ϕ$:
\begin{align}
    L \; ≡ \; &
        \frac{1}{2}
        \sum_j
        \symbf{R}_j^T
        \, \symbf{C}_j^{-1}
        \, \symbf{R}_j
\end{align}
where the residual vectors\footnote{When we use matrix notation on image-like entities, we will invariably mean that the 2-d image is flattened into a 1-d vector.} $\symbf{R}_j$ are defined as
\begin{align}
    R_j[\symbf{x}_j] \; ≡ \; &
        z_j[\symbf{x}_j]
        - \int\!\!d^2\symbf{r}
            \, ϕ_j\!\left(\symbf{A}_j\symbf{x}_j + \symbf{b}_j - \symbf{r}\right)
            \, α(\symbf{r})
    \label{eqn:residual-definition}
\end{align}
where $\symbf{r}$ are coordinates in the projection we will coadd onto, and $\symbf{A}_j$ and $\symbf{b}_j$ are the linear transform and translation that map input-image coordinates $\symbf{x}_j$ to $\symbf{r}$.
Note that we define the PSF and model in coadd coordinates, not input-image coordinates.
This likelihood is \emph{local} because we have assumed the PSF is not spatially varying and the transformation is affine -- a valid assumption in the neighborhood of any particular point, but not over larger scales.
This is a common feature of the approaches to coaddition described here: we will explicitly subdivide the sky into small cells whose maximum extent is set by the scales on which these assumptions are valid.

This likelihood is quite general in many respects:
\begin{itemize}
    \item the input pixel grid may or may not be well-sampled;
    \item pixels on the same image may have arbitrary noise correlations;
    \item pixels with no data or artifacts can simply be omitted from the sum,
    provided they can be identified.
\end{itemize}
We will need to place restrictions on these to make the method computationally tractable in later steps.
In other respects, we have made powerful simplifying assumptions:
\begin{itemize}
    \item all images have been background subtracted in advance;
    \item the true sky is completely static ($α$ is the same for every epoch);
    \item there is no wavelength dependence at all - so either the true sky is monochromatic, or all PSFs have the same throughput as a function of wavelength, so all wavelength dependency cancels.
\end{itemize}
We will relax each of these assumptions in later sections.

We now expand the quadratic log likelihood and group by powers of $α$:

\begin{align}
    L &= \frac{\kappa}{2} - \int\!\!d^2\symbf{r} \, \Psi(\symbf{r}) \, α(\symbf{r})
        +\frac{1}{2}\int\!\!d^2\symbf{r} \! \int\!\!d^2\symbf{s}
            \, \Phi(\symbf{r}, \symbf{s}) \, α(\symbf{r}) \, α(\symbf{s})
\end{align}
with
\begin{align}
    \kappa_c &\equiv \sum_j \kappa_j
        \equiv \sum_j \sum_{\symbf{x}_j} \sum_{\symbf{y}_j}
        C_j^{-1}[\symbf{x}_j, \symbf{y}_j] \; z_j[\symbf{x}_j] \, z_j[\symbf{y}_j] \\
    \Psi_c(\symbf{r}) &\equiv \sum_j \Psi(\symbf{r})
        \equiv \sum_j \sum_{\symbf{x}_j} \sum_{\symbf{y}_j}
        C_j^{-1}[\symbf{x}_j, \symbf{y}_j] \; z_j[\symbf{x}_j] \,
        ϕ_j\!\left(\symbf{A}_j\symbf{y}_j + \symbf{b}_j - \symbf{r}\right) \\
    \Phi_c(\symbf{r}, \symbf{s}) &\equiv \sum_j \Phi(\symbf{r}, \symbf{s})
        \equiv \sum_j \sum_{\symbf{x}_j} \sum_{\symbf{y}_j}
        C_j^{-1}[\symbf{x}_j, \symbf{y}_j] \;
        ϕ_j\!\left(\symbf{A}_j\symbf{x}_j + \symbf{b}_j - \symbf{r}\right) \,
        ϕ_j\!\left(\symbf{A}_j\symbf{y}_j + \symbf{b}_j - \symbf{s}\right) \\
\end{align}

$\Psi_c$ and $\Phi_c$ are sufficient statistics for our model ($\kappa_c$ does not constrain the model; it enters only if one is interested in the Bayesian evidence or other goodness-of-fit measures), and they include all sums over input images.
But as continuous functions, we can't consider them a coadd, let alone a practical one.

If we choose our output grid such that all input PSFs are well sampled, however, we can replace
\begin{align}
    ϕ_j\!\left(\symbf{A}_j\symbf{x}_j + \symbf{b}_j - \symbf{r}\right) = &
        \sum_{\symbf{p}} ϕ_j\!\left(\symbf{A}_j\symbf{x} + \symbf{b}_j - \symbf{p}\right)
            \, \sinc^2(\symbf{p} - \symbf{r})
\end{align}
and obtain
\begin{align}
    \Psi_c(\symbf{r}) &= \sum_{\symbf{p}} \Psi_c(\symbf{p}) \, \sinc^2(\symbf{r} - \symbf{p}) \\
    \Phi_c(\symbf{r}, \symbf{s}) &= \sum_{\symbf{p}} \sum_{\symbf{q}} \Phi_c(\symbf{p}, \symbf{q})
        \, \sinc^2(\symbf{r} - \symbf{p})
        \, \sinc^2(\symbf{s} - \symbf{q})
\end{align}
i.e. $\Psi_c$ and $\Phi_c$ are guaranteed to be well-sampled as well, even if the input pixel grids are not.
We are still quite far from a practical coadd, however; we want an image as well as a PSF and/or covariance matrix that enter into the likelihood in essentially the same way that a single input image does.
That means we want to solve
\begin{align}
    \Psi(\symbf{r}) &= \sum_{\symbf{p}}\sum_{\symbf{q}} C^{-1}_c[\symbf{p}, \symbf{q}]
        \, \phi_c[\symbf{r} - \symbf{p}] \, z_c[\symbf{q}] \label{eqn:brute-force-Psi}\\
    \Phi(\symbf{r}, \symbf{s}) &= \sum_{\symbf{p}}\sum_{\symbf{q}} C^{-1}_c[\symbf{p}, \symbf{q}]
        \, \phi_c[\symbf{r} - \symbf{p}] \, \phi_c[\symbf{s} - \symbf{q}]
    \label{eqn:brute-force-Phi}
\end{align}
for $\phi_c$, $C_c$, and $z_c$.
There is no single unique solution, and in fact no guarantee of any solution for completely general inputs.
This approach is one limit of the \texttt{IMCOM} method of \citet{2011ApJ...741...46R}, which utilizes a weight function that minimizes S/N losses during coaddition along with errors in PSF reconstruction; when there is no S/N loss and the PSF reconstruction is exact, their solution satisfies [\ref{eqn:brute-force-Psi}] and [\ref{eqn:brute-force-Phi}], while providing diagnostics when no such solution is possible.
\texttt{IMCOM} is also extremely computationally expensive, as is any method that relies on direct accumulation of a fully general $\Phi$.
In this technical note, we will instead focus on methods that start with simplifying assumptions that are at least \emph{mostly} true, and then investigate ways to iteratively and incrementally deal with those assumptions being weakly violated.

The first of these simplifying assumptions is that the input noise in each image is constant:\footnote{It should be possible to assume only that the noise is stationary, but the algebra is much more involved.}
\begin{align}
    C_j[\symbf{x}_j, \symbf{y}_j] \rightarrow \sigma_j^2 \, \delta[\symbf{x}_j-\symbf{y}_j]
\end{align}
A single-input contribution to $\Phi_c$ is then
\begin{align}
    \Phi_j(\symbf{r}, \symbf{s}) &= \frac{1}{\sigma_j^2} \sum_{\symbf{x}_j} \;
        ϕ_j\!\left(\symbf{A}_j\symbf{x}_j + \symbf{b}_j - \symbf{r}\right) \,
        ϕ_j\!\left(\symbf{A}_j\symbf{x}_j + \symbf{b}_j - \symbf{s}\right)
\end{align}
with Fourier transform
\begin{align}
    \widetilde{\Phi_j}(\symbf{u}, \symbf{v}) &= \frac{1}{\sigma_j^2}
        \widetilde{\phi_j}(-\symbf{u}) \, \widetilde{\phi_j}(-\symbf{v}) \,
        e^{-2\pi i \symbf{b}^T(\symbf{u}+\symbf{v})} \,
        \sum_{\symbf{x}_j} \, e^{-2\pi i \symbf{x}_j^{T} \symbf{A}_j^T (\symbf{u} + \symbf{v})}
\end{align}
To simplify further, we'd like to be able to apply the Poisson summation formula to the sum over $\symbf{x}_j$, but we need to be careful about the summation limits; so far we've implicitly (and vaguely) summed over all input pixels relevant to a cell, excluding any completely missing pixels.
To instead sum over $\mathbb{Z}^2$, we need the original sum in [\ref{eqn:residual-definition}] to truncate outside some finite region by defining both $z_j$ and $\alpha$ to be zero outside that region, and we need to assume there are no missing pixels.
Neighboring cells will necessarily define truncation inconsistently from each other, so we will also need to build the coadd for a cell with some padding, such that coadd pixels within the cell are (to good approximation) not affected by the truncation, and thus consistent with their neighbors.
How much padding will depend on the details of the algorithm, and the degree to which we aim for compactness in the real domain vs. compactness in the Fourier domain.

With these qualifications, $\widetilde{\Phi_j}$ becomes
\begin{align}
    \widetilde{\Phi_j}(\symbf{u}, \symbf{v}) &= \frac{1}{\sigma_j^2}
        \widetilde{\phi_j}(-\symbf{u}) \, \widetilde{\phi_j}(-\symbf{v}) \,
        e^{-2\pi i \symbf{b}^T(\symbf{u}+\symbf{v})} \!
        \sum_{\symbf{k}_j \in \mathbb{Z}^2}
            \delta\left(\symbf{k}_j - \symbf{A}_j^T (\symbf{u} + \symbf{v}) \right)
\end{align}

But $\phi_j$ is band-limited on the coadd pixel grid, so $\widetilde{\phi_j}$ is zero if $|\symbf{u}|_1 \ge \frac{1}{2}$ or $|\symbf{v}|_1 \ge \frac{1}{2}$, and we can assume $|\symbf{u} + \symbf{v}|_1 < 1$.
Considering the case where $\symbf{A}_j$ is a uniform scaling for simplicity:
\begin{itemize}
    \item if $|A_j| < 1$, the coadd pixels are larger than the input pixels; only the $\symbf{k}_j = 0$ term in the sum survives (note that the input image then must be well-sampled, because the coadd is).
    \item if $|A_j| > 1$, the coadd pixels are smaller than the input pixels, and other terms may survive only if $A_j$ is greater than twice the actual band limit of the PSF -- or, in other words, only if the input image is not well sampled.
\end{itemize}

Our final simplifying assumption is naturally that the input images are well-sampled; this yields
\begin{align}
    \widetilde{\Phi_j}(\symbf{u}, \symbf{v}) &= \frac{1}{\sigma_j^2}
        \widetilde{\phi_j}(-\symbf{u}) \, \widetilde{\phi_j}(-\symbf{v}) \,
        e^{-2\pi i \symbf{b}^T(\symbf{u}+\symbf{v})} \,
        \delta\left(-\symbf{A}_j^T (\symbf{u} + \symbf{v}) \right) \\
    &= \frac{1}{\sigma_j^2}
        \left|\widetilde{\phi_j}(\symbf{u})\right|^2
        \delta(\symbf{u} + \symbf{v})
    \label{eqn:phi-simplified}
\end{align}
If we then accumulate the contributions from each input image in the Fourier domain, the delta function factors out of the sum, and we can write
\begin{align}
    \widetilde{\Phi_c}(\symbf{u}, \symbf{v})
        &= \delta(\symbf{u} + \symbf{v})
            \sum_j \frac{1}{\sigma_j^2} \left|\widetilde{\phi_j}(\symbf{u})\right|^2
        = \delta(\symbf{u} + \symbf{v})
            \frac{1}{\sigma_c^2} \left|\widetilde{\phi_c}(\symbf{u})\right|^2
\end{align}
This is essentially the continuous Fourier version of [\ref{eqn:brute-force-Phi}], but here the solution is trivial, and in many respects ideal:
\begin{align}
    \widetilde{\phi_c}(\symbf{u}) &= \sigma_c \sqrt{\widetilde{\Phi_c}(\symbf{u}, \symbf{u})}
        = \sigma_c \sqrt{\sum_j \frac{1}{\sigma_j^2} \left|\widetilde{\phi_j}(\symbf{u})\right|^2} \\
    \frac{1}{\sigma_c} &= \int\!\!d^2\symbf{u} \, \sqrt{\sum_j \frac{1}{\sigma_j^2} \left|\widetilde{\phi_j}(\symbf{u})\right|^2}
\end{align}
Note that
\begin{itemize}
    \item the coadd PSF $\phi_c$ is the Fourier-domain square root of $\Phi_c$;
    \item the noise in the coadd is also constant, and its standard deviation $\sigma_c$ can be derived as the constant the normalizes the coadd PSF $\phi_c$.
\end{itemize}

With these in hand, we can compute the Fourier-domain coadd image $z_c$ from the Fourier transform of $\Psi_c$:
\begin{align}
    \widetilde{z_c}(\symbf{u})
        &= \frac{\widetilde{\Psi_c}(\symbf{u}) \, \sigma_c^2}{\widetilde{\phi_c}(\symbf{u})}
\end{align}
Obtaining $\widetilde{\Psi_c}$ from $z_j$ and $\phi_j$ involves a continuous Fourier transform of either noisy, sampled data or something derived from it; a subtly difficult task.
But it is also possible to reframe this as a linear operation in the image domain:
\begin{align}
    z_c(\symbf{r}) &= \sum_j \sum_{\symbf{x}_j} K_j\!\left(\symbf{r} - \symbf{A}_j\symbf{x}_j - \symbf{b}_j\right)
        \, z_j[\symbf{x}_j] \label{eqn:image-domain-coadd} \\
    \widetilde{K}_j(\symbf{u}) &\equiv
        \frac{
            \sigma_c^2 \, \widetilde{\phi_j^\ast}(\symbf{u})
        }{
            \sigma_j^2 \, \widetilde{\phi_c}(\symbf{u})
        }
\end{align}

This simple result still hides a lot of practical complexity, because $K$ is
\begin{itemize}
    \item generally not compact in the image domain (in fact, it is band-limited, and hence \emph{formally} must have infinite extent in the image domain);
    \item potentially expensive to evaluate at the many non-integer points implied by \ref{eqn:image-domain-coadd}.
\end{itemize}

Overall, it is unclear whether we would be best served by approach that approximates the continuous Fourier transforms with DFTs (with all of the boundary-condition and folding issues that always entails) or one that works in the image domain, perhaps with analytic approximations or basis functions in the representation of $\phi$ that have analytic Fourier transforms.
We will explore several possibilities in Section~\ref{sec:implementation-options}.

\subsection{Approximate PSFs}

In many case we may not want to use or may not be able to use the best available model of the PSF when building a coadd.
Using an approximation formally breaks the property that the coadd is a sufficient statistic for the static sky, but the actual information loss could nevertheless be quite small (much smaller than the loss due to building naive direct coadds in all cases).
In many of the implementation options we discuss later in Section~\ref{sec:implementation-options}, using an approximation with an analytic Fourier transform is required.
In other cases:
\begin{itemize}
    \item The approximation may be more compact in the Fourier and/or image domain.
    \item The approximation may be less affected by measurement noise or numerical instability, both of which can cause trouble with deconvolution operations.
    \item The true PSF may be chromatic, so even our best model evaluated with some nomimal SED is only approximately right for other SEDs.
    \item We may have already built the coadd with a PSF we now know to be subtly wrong, and want to understand systematic errors this may have introduced into later processing and/or update the coadd without starting over.
\end{itemize}

Because the coadd likelihood will no longer be exactly equivalent to the original joint single-epoch likelihood with this approximate, we will instead start with [\ref{eqn:image-domain-coadd}] as a prescription for building a coadd, but replace the true (or best-estimate) per-epoch PSFs $\phi_j$ with deliberate approximations $p_j$, and then derive the new effective coadd PSF.

We start by expanding $\widetilde{K}$ with that substition:
\begin{align}
    \widetilde{K}_j(\symbf{u}) &\equiv
        \frac{
            \sigma_c^2\, \widetilde{p_j^\ast}(\symbf{u})
        }{
            \sigma_j^2 \, \sqrt{\sum\limits_n \frac{1}{\sigma_n^2}\left|\widetilde{p_n}(\symbf{u})\right|^2}
        }
    \label{eqn:adjusted-k}
\end{align}
Note that $\sigma_c$ is at this stage just an unknown normalization constant; we will show later that it is equal to the noise on the coadd when the effective coadd PSF is normalized.

To compute the effective PSF of the coadd, we coadd the image of a point source centered at $\symbf{r}=0$ using [\ref{eqn:image-domain-coadd}]:
\begin{align}
    \phi_c(\symbf{r}) &= \sum_j \sum_{\symbf{x}_j} K_j\!\left(\symbf{r} - \symbf{A}_j\symbf{x}_j - \symbf{b}_j\right) \, \phi_j\!\left(\symbf{A}_j\symbf{x}_j + \symbf{b}_j\right)
\end{align}
It is important here that we use the true PSFs $\phi_j$ (or our best-available model).
We are also continuing to rely on our assumption that all input images are well-sampled - without that, $K$ would need to be a position-dependent kernel to have a well-defined PSF on the coadd.
That assumption also lets us perform this convolution in coadd coordinates\footnote{TODO: prove this by a Fourier round-trip; we get another sum where only $\symbf{k}=0$ survives.}:
\begin{align}
    \phi_c(\symbf{r}) &= \sum_j \frac{1}{|A_j|} \sum_{\symbf{s}} K_j\!\left(\symbf{r} - \symbf{s}\right) \, \phi_j\!\left(\symbf{s}\right)
\end{align}
We can now solve for $\sigma_c$ by requiring
\begin{align}
    \int\! \phi_c(\symbf{r}) \, d^2 \symbf{r} &= 1
\end{align}

The noise covariance of the coadd can be computed via straightforward uncertainty propagation:
\begin{align}
    C_c(\symbf{r}, \symbf{s}) &= \sum_j \sigma_j^2 \sum_{\symbf{x}_j} \,
        K_j\!\left(\symbf{r} - \symbf{A}_j\symbf{x}_j - \symbf{b}_j\right) \,
        K_j\!\left(\symbf{s} - \symbf{A}_j\symbf{x}_j - \symbf{b}_j\right)
\end{align}
Its Fourier transform is
\begin{align}
    \widetilde{C_c}(\symbf{u}, \symbf{v}) &= \sum_j \sigma_j^2 \sum_{\symbf{x}_j} \,
        \widetilde{K_j}(\symbf{u}) \, \widetilde{K_j}(\symbf{v}) \,
        e^{-2\pi i \symbf{b}_j^T(\symbf{u} + \symbf{v})}
        \sum_{\symbf{x}_j} \, e^{-2\pi i \symbf{x}_j^{T} \symbf{A}_j^T (\symbf{u} + \symbf{v})} \\
    &= \sum_j \sigma_j^2 \, \left|\widetilde{K_j}(\symbf{u}) \right|^2 \delta(\symbf{u} + \symbf{v})
\end{align}
where the simplification comes from using the Poisson summation formula and rejecting modes other than $\symbf{k}=0$, just as in [\ref{eqn:phi-simplified}].
Substituting the definition of $\widetilde{K}$ from [\ref{eqn:adjusted-k}], this simplifies further to
\begin{align}
    \widetilde{C_c}(\symbf{u}, \symbf{v}) &=
        \left[\sum_j \frac{\sigma_c^2}{\sigma_j^2} \left|\widetilde{p_j}(\symbf{u})\right|^2\right]
        \left[\sum_n \frac{1}{\sigma_n^2} \left|\widetilde{p_n}(\symbf{u})\right|^2\right]^{-1}
        = \sigma_c^2
\end{align}
Even with the PSF approximated, the noise in the coadd is uncorrelated and stationary.


\section{Implementation options}

\label{sec:implementation-options}

\subsection{Resampling in advance}

In this section, we consider implementation options that assume all input images have already been resampled to the coadd grid in advance, presumably via a compact image-domain interpolant that approximates $\sinc$ (e.g. Lanczos).
This inevitably introduces aliasing, which is traditionally ignored; we will ignore it here as well, but do consider it to be one of the disadvantages of these approaches.

\subsubsection{Native-resolution FFT convolution and deconvolution}

In this approach, input data images $z_j$ are first zero-padded and FFT'd.
Best-available PSF models $\phi_j$ and approximations $p_j$ are either rendered directly in the Fourier domain or rendered as images and also FFT'd onto the same (discrete) Fourier grid.
We then accumulate the following sums:
\begin{align}
    \widetilde{\Psi_c}[\symbf{u}] &=
        \sum_j \frac{\widetilde{p_j^*}[\symbf{u}]\,\widetilde{z_j}[\symbf{u}]}{\sigma_j^2} \\
    \widetilde{\Phi_c^{(1)}}[\symbf{u}] &=
        \sum_j \frac{\widetilde{p_j^*}[\symbf{u}]\,\widetilde{\phi_j}[\symbf{u}]}{\sigma_j^2} \\
    \widetilde{\Phi_c^{(2)}}[\symbf{u}] &=
        \sum_j \frac{\widetilde{p_j^*}[\symbf{u}]\,\widetilde{p_j}[\symbf{u}]}{\sigma_j^2}
\end{align}
The unnormalized coadd PSF model $\widetilde{\phi_c}$ is then
\begin{align}
    \frac{\widetilde{\phi_c}[\symbf{u}]}{\sigma_c^2} &= \frac{
        \widetilde{\Phi_c^{(1)}}[\symbf{u}]
    }{
        \sqrt{\widetilde{\Phi_c^{(2)}}[\symbf{u}]}
    } \, ;
\end{align}
we can then inverse FFT and solve for $\sigma_c$ via
\begin{align}
    \sum_{\symbf{r}} \phi_c[\symbf{r}] &= 1 \, .
\end{align}

Finally, the Fourier-domain coadd image is
\begin{align}
    \widetilde{z_c}[\symbf{u}] &= \frac{
        \widetilde{\Psi_c}[\symbf{u}] \, \sigma_c^2
    }{
        \sqrt{\widetilde{\Phi_c^{(2)}}[\symbf{u}]}
    } \, ,
\end{align}
which we also inverse FFT to form the coadd image.

This approach is in many respects the simplest possible implementation, especially if one already has an optimized pipeline for image resampling.
It is also probably among the fastest: there are no expensive image-domain convolutions, and the padding necessary for the FFT is \emph{probably} only proportional to the PSF size, not the size of the coadd cell.

The downside of this approach is its nonlocality in the image domain, especially its vulnerability to edge effects.
If a source lands on the edge of the image we want to FFT, zero-padding will cause a sharp edge that strongly violates the assumption that the image is overall band-limited, yielding aliasing in the FFT.
Some form of apodization of these edges thus seems to be necessary in practice, such as Gaussian process interpolation (with correlation length set by the PSF) between the image data and the zero-padding region.
If our convolution operations were compact in the image domain, we could strictly guarantee that these artificial pixels had no effect on the final coadd itself, by including enough \emph{data} padding around the region of interest within the region of apodization.
But the FFT division by $\sqrt{\widetilde{\Phi_c^{(2)}}[\symbf{u}]}$ in particular combines information from all pixels in the image domain, when computing the value of any other pixel.
The same kinds of problems can also come up with interpolating missing pixels within the target coadd cell; any problems in those pixels end up affecting (at some level) all pixels.

Attempting the apodization anyway seems like the only way forward; it may be that it works well enough, and the operations are ``local enough'' in the image domain for any systematic errors introduced to be small.
In any case, this is probably an experimental question; the challenges come from the kids of messy situations that are often hard to simulate, let alone analyze mathematically.

\subsubsection{Analytic single-kernel image-domain convolution}

\subsection{Combining resampling and weights}

\subsubsection{GALSIM-style Fourier-domain accumulation}

\subsubsection{Analytic single-kernel image-domain resampling and weights}

\section{Extensions for relaxed assumptions}

\subsection{Nonstationary noise}

\subsection{Undersampled input images}

\subsection{Missing pixels}

\subsection{Chromatic PSFs}


\appendix
% Include all the relevant bib files.
% https://lsst-texmf.lsst.io/lsstdoc.html#bibliographies
\section{References} \label{sec:bib}
\renewcommand{\refname}{} % Suppress default Bibliography section
\bibliography{local,lsst,lsst-dm,refs_ads,refs,books}

% Make sure lsst-texmf/bin/generateAcronyms.py is in your path
\section{Acronyms} \label{sec:acronyms}
\input{acronyms.tex}
% If you want glossary uncomment below -- comment out the two lines above
%\printglossaries





\end{document}
